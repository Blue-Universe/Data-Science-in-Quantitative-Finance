{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Probelm 1: LSA-based Recommender\n",
    "- Work off of the LSA notebook that we went through in class\n",
    "- Use the Reuters 10K article corpus raw_text_dataset.pickle (obtained from https://github.com/chrisjmccormick/LSA_Classification)\n",
    "    \n",
    "- Create a doc2vec(doc, tfidf_vectorizer) function corresponding to a TFIDF vectorizere\n",
    "    - INPUTS: doc, tfidf_vectorizer\n",
    "        - doc - any string\n",
    "        - tfidf_vectorizer - a TfidfVectorizer instance\n",
    "    - OUTPUTS: vec, doc_features, doc_counts\n",
    "        - vec - a vector with $L_2$ norm of $1$\n",
    "        - doc_features - the features after tokenization and pre-processing\n",
    "        - doc_counts - the counts of each feature in this document\n",
    "    - train tfidf_vectorizer on the Reuters 10K article corpus\n",
    "- For each of the following doc strings, calculate their corresponding vectors\n",
    "    - doc1: \"The cocoa cadabra\"\n",
    "    - doc2: \"AAPL SE\"\n",
    "    - doc3: \"bullish stocks\"\n",
    "    - doc3: \"I walked through a random forest and earned a high premium\"\n",
    "- Create a **recommend(vec, X_model, X_corpus)** function:\n",
    "    - which projects any document vector onto a given X_model\n",
    "        - here X_model = {X_train_tfidf, and X_train_lsa}\n",
    "    - and returns doc_vec, idx_top10, sim_top10, X_top10 as follows\n",
    "        - doc_vec - the (sparse) vector of similarity scores of vec and members of X_model. \n",
    "        This vector should be size (Dx1)\n",
    "        - idx_top10: the indices of the top-10 similarity scores\n",
    "        - sim_top10: the top-10 similarity scores\n",
    "        - X_top10: the top-10 corpus articles most similar to the input model\n",
    "    - what does your recommend() function ouput for the doc vectors in the previous exersise?\n",
    "        - Do you see an improvement of the LSA similarity recommendation relative to the TF-IDF similarity recommendation?\n",
    "        \n",
    "- BONUS:\n",
    "    - repeat the same exercise but instead of the Reuters 10K dataset, use this corpus of 200K English plaintext jokes https://github.com/taivop/joke-dataset . Does your recommender system actually find similar jokes? Give examples of good recommendations and bad recommendations. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a doc2vec(doc, tfidf_vectorizer) function corresponding to a TFIDF vectorizere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(doc, tfidf_vectorizer):\n",
    "    \"\"\"A transform function\n",
    "    This function transforms documnet to a tfidf vector\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    doc : string\n",
    "          The document you would like to transform.\n",
    "    \n",
    "    tfidf_vectorizer : sklearn.feature_extraction.text.TfidfVectorizer\n",
    "                       a trained TfidfVectorizer according to which transfrom doc to vector\n",
    "                       \n",
    "    Returns\n",
    "    -------\n",
    "    vec : parse vector\n",
    "          the vector doc transformed into.\n",
    "              \n",
    "    doc_features : list of strings\n",
    "                   The vocabulary of tfidf_vectorizer\n",
    "                \n",
    "    doc_counts : np.array\n",
    "                 the counts of each feature in this document\n",
    "    \n",
    "    \"\"\"\n",
    "    doc_tfidf = tfidf_vectorizer.transform([doc])\n",
    "    vec = doc_tfidf.toarray().ravel()\n",
    "    doc_features = tfidf_vectorizer.get_feature_names()\n",
    "    Count_vectorizer = CountVectorizer()\n",
    "    Count_vectorizer.vocabulary_ = {k:i for i,k in enumerate(tfidf_vectorizer.get_feature_names())}\n",
    "    doc_counts = Count_vectorizer.transform([doc]).toarray().ravel()\n",
    "    return vec, doc_features, doc_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train tfidf_vectorizer on the Reuters 10K article corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"raw_text_dataset.pickle\"\n",
    "filepath = os.getcwd() + '/' + fname\n",
    "raw_text_dataset = pickle.load(open(filepath, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5, # ignore terms which occur in more than half of the documents\n",
    "    max_features=10000,\n",
    "    min_df=2, # ignore terms which occur in less than 2 documents\n",
    "    stop_words='english',\n",
    "    norm='l2',\n",
    "    use_idf=True, \n",
    "    analyzer='word',\n",
    "#     token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
    "    token_pattern = '(?u)\\\\b[a-zA-Z]\\\\w+\\\\b'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = raw_text_dataset[0] + raw_text_dataset[2] \n",
    "corpus_tfidf = tfidf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate their corresponding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"The cocoa cadabra\"\n",
    "doc2 = \"AAPL SE\"\n",
    "doc3 = \"bullish stocks\"\n",
    "doc4 = \"I walked through a random forest and earned a high premium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorcalculation(doc,tfidf_vectorizer = tfidf_vectorizer):\n",
    "    vec, doc_features, doc_counts = doc2vec(doc, tfidf_vectorizer)\n",
    "    index = np.where(vec)[0]\n",
    "    \n",
    "    for i in index:\n",
    "        print (\"word : \", doc_features[i], \" tfidf : \", vec[i], \"numbers : \",doc_counts[i])\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the result of word  1\n",
      "----------\n",
      "word :  cocoa  tfidf :  1.0 numbers :  1\n",
      "\n",
      "Below are the result of word  2\n",
      "----------\n",
      "word :  aapl  tfidf :  0.7267972294274684 numbers :  1\n",
      "word :  se  tfidf :  0.6868520854569459 numbers :  1\n",
      "\n",
      "Below are the result of word  3\n",
      "----------\n",
      "word :  bullish  tfidf :  0.8130834300057836 numbers :  1\n",
      "word :  stocks  tfidf :  0.5821471771382473 numbers :  1\n",
      "\n",
      "Below are the result of word  4\n",
      "----------\n",
      "word :  earned  tfidf :  0.3417258810296154 numbers :  1\n",
      "word :  forest  tfidf :  0.4421975166235162 numbers :  1\n",
      "word :  high  tfidf :  0.2477341391645119 numbers :  1\n",
      "word :  premium  tfidf :  0.3478755645367025 numbers :  1\n",
      "word :  random  tfidf :  0.4947816947727578 numbers :  1\n",
      "word :  walked  tfidf :  0.5103785271100408 numbers :  1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in [1,2,3,4]:\n",
    "    print (\"Below are the result of word \", i)\n",
    "    print (\"----------\")\n",
    "    doc = globals () [\"doc\"+str(i)]\n",
    "    globals () [\"vec\"+str(i)] = vectorcalculation(doc,tfidf_vectorizer)\n",
    "    print (\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The corresponding vectors has been stored in variable vec1, vec2, vec3,vec4\n",
    "# For example we could have a look at the vector \"The cocoa cadabra\" coresponds to.\n",
    "vec1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a recommend(vec, X_model, X_corpus) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(vec, X_model, X_corpus):\n",
    "    \"\"\" A recommend system.\n",
    "    \n",
    "    This recommend system \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vec : array-like\n",
    "          Any document vector projected onto a given X_model.\n",
    "          \n",
    "    X_model : {X_train_tfidf, X_train_lsa}\n",
    "              The model vec and X_corpus is projected onto.\n",
    "              \n",
    "    X_corpus : a list of strings\n",
    "               The original articles.\n",
    "               \n",
    "               \n",
    "               \n",
    "    Returns\n",
    "    -------\n",
    "    doc_vec : parse vector, size = (Dx1) \n",
    "              Similarity scores of vec and members of X_model.\n",
    "              \n",
    "    idx_top10 : list of ints, len = 10\n",
    "                The indices of the top-10 similarity scores.\n",
    "                \n",
    "    sim_top10 : list of double, len = 10\n",
    "                The top-10 similarity scores\n",
    "                \n",
    "    X_top10 : list of strings, len = 10\n",
    "              The top-10 corpus articles most similar to the input model\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    doc_vec = euclidean_distances(vec,X_model).flatten()\n",
    "    idx_top10 = np.argsort(doc_vec.flatten())[0:10]\n",
    "    sim_top10 = doc_vec[idx_top10]\n",
    "    X_top10 = []\n",
    "    for i in idx_top10:\n",
    "        X_top10.append(X_corpus[i])\n",
    "    \n",
    "    return doc_vec, idx_top10, sim_top10, X_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### what does your recommend() function ouput for the doc vectors in the previous exersise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_train = raw_text_dataset[0]\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project the tfidf vectors onto the first N principal components.\n",
    "# Though this is significantly fewer features than the original tfidf vector,\n",
    "# they are stronger features, and the accuracy is higher.\n",
    "svd = TruncatedSVD(\n",
    "    n_components=200,\n",
    "    random_state=42,\n",
    "    algorithm='arpack'\n",
    ")\n",
    "\n",
    "lsa = make_pipeline(\n",
    "    svd, \n",
    "#     Normalizer(copy=False) # try commenting this out. Do you get a better result?\n",
    ")\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_vector_recommend(doc, X_model , method ,X_corpus = corpus_train, tfidf_vectorizer = tfidf_vectorizer,lsa=lsa):\n",
    "    if method == \"tfidf\" :\n",
    "        vec = tfidf_vectorizer.transform([doc])\n",
    "    elif method == \"lsa\" : \n",
    "        vec = lsa.transform(tfidf_vectorizer.transform([doc]))\n",
    "    doc_vec, idx_top10, sim_top10, X_top10 = recommend(vec, X_model, X_corpus)\n",
    "    return doc_vec, idx_top10, sim_top10, X_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [1,2,3,4]:\n",
    "    doc = globals () [\"doc\"+str(i)]\n",
    "    for j in [\"tfidf\",\"lsa\"]:\n",
    "        if j == \"tfidf\":\n",
    "            X_model = X_train_tfidf\n",
    "        elif j == \"lsa\":\n",
    "            X_model = X_train_lsa\n",
    "        doc_vec, idx_top10, sim_top10, X_top10 = doc_vector_recommend(doc, X_model , method = j)\n",
    "        globals () [\"doc_vec\"+\"_\"+str(i)+\"_\"+j] = doc_vec\n",
    "        globals () [\"idx_top10\"+\"_\"+str(i)+\"_\"+j] = idx_top10\n",
    "        globals () [\"sim_top10\"+\"_\"+str(i)+\"_\"+j] = sim_top10\n",
    "        globals () [\"X_top10\"+\"_\"+str(i)+\"_\"+j] = X_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ouputs has been stored in the variables like doc_vec_1_tfidf, which means the similarity score between document1 and corpus articles under tfidf model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let look at the the documentation which is most similar to \"bullish stocks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUBROTO SEES OIL MARKET CONTINUING BULLISH\\n\\nIndonesian Energy Minister Subroto said he sees the oil market continuing bullish, with underlying demand expected to rise later in the year. He told a press conference in Jakarta at the end of a two-day meeting of South-East Asian Energy Ministers that he saw prices stabilizing around 18 dlrs a barrel. \"The sentiment in the market is bullish and I think it will continue that way as demand will go up in the third or fourth quarters,\" Subroto said. Asked about the prospect for oil prices, he said: \"I think they will stabilise around 18 dlrs, although there is a little turbulence ...\" \"Of course the spot price will fluctuate, but the official price will remain at 18 dlrs,\" he added. REUTER '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_top10_3_tfidf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we firstly find the index of the documentation which is most similar to \"bullish stocks\", then use the index to get the documentation, we get the same result as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUBROTO SEES OIL MARKET CONTINUING BULLISH\\n\\nIndonesian Energy Minister Subroto said he sees the oil market continuing bullish, with underlying demand expected to rise later in the year. He told a press conference in Jakarta at the end of a two-day meeting of South-East Asian Energy Ministers that he saw prices stabilizing around 18 dlrs a barrel. \"The sentiment in the market is bullish and I think it will continue that way as demand will go up in the third or fourth quarters,\" Subroto said. Asked about the prospect for oil prices, he said: \"I think they will stabilise around 18 dlrs, although there is a little turbulence ...\" \"Of course the spot price will fluctuate, but the official price will remain at 18 dlrs,\" he added. REUTER '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[idx_top10_3_tfidf[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us calculate the min of similar score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1799903470933173"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vec_3_tfidf.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could get the same result if we find the first element of the the top-10 similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1799903470933173"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_top10_3_tfidf[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do you see an improvement of the LSA similarity recommendation relative to the TF-IDF similarity recommendation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GHANA COCOA PURCHASES FALL, CUMULATIVE STILL UP\\n\\nThe Ghana Cocoa Board said it purchased 1,323 tonnes of cocoa in the 21st week, ended February 26, of the 1986/87 main crop season, compared with 1,961 tonnes the previous week and 1,344 tonnes in the 21st week ended March six of the 1985/86 season, the board said. Cumulative purchases so far this season stand at 216,095 tonnes, still up on the 201,966 tonnes purchased by the 21st week of last season, the Board said. Reuter '"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_top10_1_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CALNY INC CLNY> SUES PEPSICO INC PEP>\\n\\nCalny Inc said it has filed a multi-million-dlr suit against PepsiCo Inc and its La Petite Boulangerie unit. Calny, which holds 15 La Petite Boulangerie franchises, alleges it and PepsiCo breached their agreements with Calny by failing to support the franchises in a number of ways. The company further alleges that PepsiCo and La Petite Boulangerie had fiduciary responsibilities to Calny because of the longstanding relationship between Calny and Taco Bell, also a PepsiCo subsidiary. Calny operates 143 Taco Bell restaurants. Calny said Pepsico misrepresented the readiness of the La Petite Boulangerie to expand outside San Francisco and misrepresented costs involved in operating the restaurants. Reuter '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_top10_1_lsa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'APPLE COMPUTER AAPL> UPGRADES MACINTOSH LINE\\n\\nApple Computer Inc today will announce the addition of two new machines to its profitable Macintosh line of personal computers, both aimed at the business market. The Macintosh was first introduced in January 1984 and has been upgraded several times since then. Both of the new machines, the Macintosh SE and the Macintosh II, will be faster and more versatile, but considerably more expensive than earlier models. The Mac SE (SE stands for \"system expansion\"), which Apple says will operate 15-20 pct faster than its current Mac Plus, goes on sale today. It carries a suggested retail price ranging from 2,899 to 3,699 dlrs depending on its features. The Mac II, designed to run about four times faster than the Mac Plus, is to be ready for shipping in May and priced between 4,798 and 6,998 dlrs. Mac Plus, which went on the market one year ago, sells for about 2,200 dlrs. Both new computers are to be unveiled at the AppleWorld Conference in Los Angeles. Company officials expressed high hopes for both computers at a press briefing on Friday, especially the high-performance Mac II which is designed to give Apple an entree to the expanding market for science and engineering workstations. John Sculley, Apple chairman and chief executive officer, declined to estimate anticipated sales, but he said the Mac SE should contribute significantly to Apple\\'s bottom line this year. He said it would appeal to the mainstream of PC users. \"I believe the Mac SE will be the product of choice for most people,\" he said. \"My sense is that it will be a real power product for revenue.\" Bruce Lupatkin, senior technology analyst with Hambrecht Quist in San Francisco, said he had not seen the new computers but expected the new products to do well. \"Apple has recognized the need for a convergence of computer functions into one general all-purpose workstation,\" he told Reuters. \"The graphics interface on the Mac products is significantly better than anything IBM has to date.\" International Business Machines is expected to announce updated personal computers this spring. The Mac II uses the new Motorola 68020 microprocessor, an \"open architecture\" that allows for the addition of numerous peripheral devices, a built-in hard disk and one megabyte of memory, expandable to eight megabytes. It can be equipped with a 12-inch monochrome or a 13-inch color monitor. In a demonstration of its speed and power, company executives said they thought the Mac II would push the development of software for Apple computers in new directions that could include sophisticated video editing, electronic mail systems and sound reproduction suitable for studio use. The Mac II can be upgraded so that its monitor displays 256 colors or shades of gray. The Mac SE is built around the 68000 microprocessor and will be shipped with one megabyte RAM, expandable to four megabytes, and a nine-inch monochrome screen. Both new computers have two optional keyboards, a new feature in the Apple line of products. Reuter '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_top10_2_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CALNY INC CLNY> SUES PEPSICO INC PEP>\\n\\nCalny Inc said it has filed a multi-million-dlr suit against PepsiCo Inc and its La Petite Boulangerie unit. Calny, which holds 15 La Petite Boulangerie franchises, alleges it and PepsiCo breached their agreements with Calny by failing to support the franchises in a number of ways. The company further alleges that PepsiCo and La Petite Boulangerie had fiduciary responsibilities to Calny because of the longstanding relationship between Calny and Taco Bell, also a PepsiCo subsidiary. Calny operates 143 Taco Bell restaurants. Calny said Pepsico misrepresented the readiness of the La Petite Boulangerie to expand outside San Francisco and misrepresented costs involved in operating the restaurants. Reuter '"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_top10_2_lsa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SUBROTO SEES OIL MARKET CONTINUING BULLISH\\n\\nIndonesian Energy Minister Subroto said he sees the oil market continuing bullish, with underlying demand expected to rise later in the year. He told a press conference in Jakarta at the end of a two-day meeting of South-East Asian Energy Ministers that he saw prices stabilizing around 18 dlrs a barrel. \"The sentiment in the market is bullish and I think it will continue that way as demand will go up in the third or fourth quarters,\" Subroto said. Asked about the prospect for oil prices, he said: \"I think they will stabilise around 18 dlrs, although there is a little turbulence ...\" \"Of course the spot price will fluctuate, but the official price will remain at 18 dlrs,\" he added. REUTER '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_top10_3_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CALNY INC CLNY> SUES PEPSICO INC PEP>\\n\\nCalny Inc said it has filed a multi-million-dlr suit against PepsiCo Inc and its La Petite Boulangerie unit. Calny, which holds 15 La Petite Boulangerie franchises, alleges it and PepsiCo breached their agreements with Calny by failing to support the franchises in a number of ways. The company further alleges that PepsiCo and La Petite Boulangerie had fiduciary responsibilities to Calny because of the longstanding relationship between Calny and Taco Bell, also a PepsiCo subsidiary. Calny operates 143 Taco Bell restaurants. Calny said Pepsico misrepresented the readiness of the La Petite Boulangerie to expand outside San Francisco and misrepresented costs involved in operating the restaurants. Reuter '"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_top10_3_lsa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"NORANDA TO SPIN OFF FOREST SUBSIDIARIES\\n\\nNoranda Inc> said the company planned a public share offer within three months of its Noranda Forest Inc unit, which holds Noranda's forest products interests. Size of the offer is still undetermined, Noranda said. Noranda said Noranda Forest would operate as a freestanding subsidiary of Noranda. Noranda Forest holds 100 pct of Fraser Inc, James Maclaren Industries and Noranda Forest Sales and 50 pct stakes in MacMillan Bloedel Ltd MMBLF> and Northwood Pulp and Timber. Noranda Forest's consolidated 1986 revenues were more than three billion dlrs, with earnings of 158 mln dlrs. \""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_top10_4_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ANGOLA, URUGUAY ESTABLISH DIPLOMATIC RELATIONS\\n\\nAngola and Uruguay have established diplomatic relations at the ambassadorial level, according to a joint communique signed by their U.N. representatives and circulated here today. Reuter '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_top10_4_lsa[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I read the documents of each method recommand to me. For the first two, TF-IDF performs better and for the third one, LSA performs better. And they both done very poor on the random walk example. So, there is not an improvement of the LSA relative to TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reddit_jokes.json\") as f:\n",
    "    plaintext_jokes = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_doc = []\n",
    "for i in plaintext_jokes:\n",
    "    \n",
    "    joke_doc.append(i['title']+' '+i['body'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_tfidf = tfidf_vectorizer.fit_transform(joke_doc)\n",
    "jokes_lsa = lsa.fit_transform(jokes_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why was the cheesemaker lopsided? Because he only had one Stilton!'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vector_recommend(doc3, jokes_tfidf , \"tfidf\" ,joke_doc, tfidf_vectorizer = tfidf_vectorizer,lsa=lsa)[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What do you call Nitrogen after the sunrises? Daytrogen.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vector_recommend(doc3, jokes_lsa , \"lsa\" ,joke_doc, tfidf_vectorizer = tfidf_vectorizer,lsa=lsa)[3][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this system does not work as expected. It seems has nothing to do with bullish stocks. And these are bad examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why was the cheesemaker lopsided? Because he only had one Stilton!'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_vector_recommend(\"cheesemaker\", jokes_tfidf , \n",
    "                     \"tfidf\" ,joke_doc, tfidf_vectorizer = tfidf_vectorizer,lsa=lsa)[3][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
